{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCZYXwtCsL_y"
   },
   "source": [
    "CA02: This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm. \n",
    "\n",
    "In this assignment you will ...\n",
    "1. Complete the code such a way that it works correctly with this given parts of the program.\n",
    "2. Explain as clearly as possible what each part of the code is doing. Use \"Markdown\" texts and code commenting to explain the code\n",
    "\n",
    "IMPORTANT NOTE:\n",
    "\n",
    "The path of your data folders 'train-mails' and 'test-mails' must be './train-mails' and './test-mails'. This means you must have your .ipynb file and these folders in the SAME FOLDER in your laptop or Google Drive. The reason for doing this is, this way the peer reviewes and I would be able to run your code from our computers using this exact same relative path, irrespective of our folder hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4p_DvtT7sOIr",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the make_Dictionary function starts by creating an empty list called all_words. It then creates another list called emails that uses a list comprehension that iterates over the files and directories in the given root_dir path, joining the files names with the root_dir to get the full file path. Then in a for loop the function iterates over each file path in that emails list and opens them. Another for loop occurs where it iterates over each line in the file object and splits it at each whitespace. The split line is then added to the list all_words we made at the beginning of the function and the for loop continues for all lines.\n",
    "\n",
    "The all_words list is then fed to the Counter method where the occurences of unique words are counted and stored in a dictionary, with the key being the unique word and the value being the frequency of those words. This dictionary is then converted into a list using the list() function and assigned to the variable list_to_remove. A for loop then iterates through each item in the list, checking if it is alphabetical. If it is not then it is deleted from the dictionary. If the length of the item is 1 then it is also deleted from the dictionary. The dictionary is then converted in to a list of tuples of the 3000 most frequent words with the .most_common() function. The end result is a list of tuples with a word and the frequency of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jjKF0nIMwz8_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_Dictionary(root_dir):\n",
    "  all_words = []\n",
    "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
    "  for mail in emails:\n",
    "    with open(mail) as m:\n",
    "      for line in m:\n",
    "        words = line.split()\n",
    "        all_words += words\n",
    "  dictionary = Counter(all_words)\n",
    "  list_to_remove = list(dictionary)\n",
    "\n",
    "  for item in list_to_remove:\n",
    "    if item.isalpha() == False:\n",
    "      del dictionary[item]\n",
    "    elif len(item) == 1:\n",
    "      del dictionary[item]\n",
    "  dictionary = dictionary.most_common(3000)\n",
    "  return dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a list of file paths is made by iterating through the given directory and joining the path with the file name in a list comprehension. Then a numpy array is created with a number of rows equal to the length of the files list and 3000 columns. Another array is created with zeros, equaling the length of the files list. Then the variables count and docID are set to 1 and 0 respectively that will be used through the iterations. A for loop that first looks at individual files in the files list and then opens them iterates through the index of the lines in the file and the content of the file. If the index is 2 (because the actual content of the email starts at line 3) then it splits the content of the line into a list labeled words. Each item in words is then iterated through with a variable wordID being set to zero. Then the tuple index and content in the dictionary variable we created before is iterated through. We check to see if the word in the tuple (which is at index 0 in the tuple) is equal to the word in the words list. If it is then we set wordID to the index of that tuple and we set the instance at features_matrix[docID, wordID] = the count of that word in the email. We then initialize the instance at train_labels[docID] = 0, split the file path at each /, and look at the last item in the filepathTokens list. If it is spmsg then we set the train_labels[docID] = 1 and we increment count (which counts the number of spam messages). Then we increment docID and go back through all the for loops for all the files in the files list. At the end of the function we return the completed features_matrix and the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dmVW5xNlyOFc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(mail_dir):\n",
    "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "  features_matrix = np.zeros((len(files),3000))\n",
    "  train_labels = np.zeros(len(files))\n",
    "  count = 1;\n",
    "  docID = 0;\n",
    "  for fil in files:\n",
    "    with open(fil) as fi:\n",
    "      for i, line in enumerate(fi):\n",
    "        if i == 2:\n",
    "          words = line.split()\n",
    "          for word in words:\n",
    "            wordID = 0\n",
    "            for i, d in enumerate(dictionary):\n",
    "              if d[0] == word:\n",
    "                wordID = i\n",
    "                features_matrix[docID,wordID] = words.count(word)\n",
    "      train_labels[docID] = 0;\n",
    "      filepathTokens = fil.split('/')\n",
    "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
    "      if lastToken.startswith(\"spmsg\"):\n",
    "        train_labels[docID] = 1;\n",
    "        count = count + 1\n",
    "      docID = docID + 1\n",
    "  return features_matrix, train_labels                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zoq-rE7Mx0pp",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Enter the \"path\" of your \"train_mails\" and \"test-mails\" FOLDERS in this cell ...\n",
    "# for example: TRAIN_DIR = '../../train-mails'\n",
    "#              TEST_DIR = '../../test-mails'\n",
    "\n",
    "TRAIN_DIR = '/Users/tommyropp/Desktop/MSBA/Semester Two/BSAN 6070/bsan_6070/CA 02/Data/train-mails'\n",
    "TEST_DIR = '/Users/tommyropp/Desktop/MSBA/Semester Two/BSAN 6070/bsan_6070/CA 02/Data/test-mails'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 127480,
     "status": "ok",
     "timestamp": 1578886833446,
     "user": {
      "displayName": "Arin Brahma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBXGIW7FvUnbm_QmEFGh4rLebuLHNZgc8PuNinU=s64",
      "userId": "05299564422021375910"
     },
     "user_tz": 480
    },
    "id": "134lmhauyQxE",
    "outputId": "83cce6a6-aff5-4e93-ef0a-700606437aa9"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/train-mails'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m make_Dictionary(TRAIN_DIR)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreading and processing emails from TRAIN and TEST folders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m features_matrix, labels \u001b[38;5;241m=\u001b[39m extract_features(TRAIN_DIR)\n",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m, in \u001b[0;36mmake_Dictionary\u001b[0;34m(root_dir)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_Dictionary\u001b[39m(root_dir):\n\u001b[1;32m      2\u001b[0m   all_words \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m   emails \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir,f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(root_dir)]\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m mail \u001b[38;5;129;01min\u001b[39;00m emails:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(mail) \u001b[38;5;28;01mas\u001b[39;00m m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/train-mails'"
     ]
    }
   ],
   "source": [
    "dictionary = make_Dictionary(TRAIN_DIR)\n",
    "\n",
    "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
    "features_matrix, labels = extract_features(TRAIN_DIR)\n",
    "test_features_matrix, test_labels = extract_features(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 127480,
     "status": "ok",
     "timestamp": 1578886833446,
     "user": {
      "displayName": "Arin Brahma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBXGIW7FvUnbm_QmEFGh4rLebuLHNZgc8PuNinU=s64",
      "userId": "05299564422021375910"
     },
     "user_tz": 480
    },
    "id": "134lmhauyQxE",
    "outputId": "83cce6a6-aff5-4e93-ef0a-700606437aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model using Multinomial Naïve Bayes algorithm .....\n",
      "Training completed\n",
      "Testing trained model to predict Test Data labels\n",
      "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
      "0.9615384615384616\n"
     ]
    }
   ],
   "source": [
    "# In this section enter your code to TRAIN the model using Naive Bayes algorithm, then PREDICT and then evaluate PERFORMANCE (Accuracy)\n",
    "# Your code below ...\n",
    "#\n",
    "#\n",
    "#\n",
    "# Your output should look like below if your code is right\n",
    "\n",
    "print(\"Training Model using Multinomial Naïve Bayes algorithm .....\")\n",
    "model = MultinomialNB()\n",
    "model.fit(features_matrix, labels)\n",
    "print(\"Training completed\")\n",
    "print(\"Testing trained model to predict Test Data labels\")\n",
    "predicted_labels = model.predict(test_features_matrix)\n",
    "print(\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5_mPrvN586A"
   },
   "source": [
    "======================= END OF PROGRAM ========================="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOaSi3qlFUlqTup/1esXCKD",
   "collapsed_sections": [],
   "name": "naive_bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
